{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "optimum-click",
   "metadata": {},
   "source": [
    "# Chapter 20 - Amazon's DeepAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-focus",
   "metadata": {},
   "source": [
    "## Listing 20-1. Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "y = pd.read_csv('air_visit_data.csv.zip')\n",
    "y = y.pivot(index='visit_date', columns='air_store_id')['visitors']\n",
    "y = y.fillna(0)\n",
    "y = pd.DataFrame(y.sum(axis=1))\n",
    "\n",
    "y = y.reset_index(drop=False)\n",
    "y.columns = ['date', 'y']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-zambia",
   "metadata": {},
   "source": [
    "## Listing 20-2. Preparing the data format requered by the gluonts library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.common import ListDataset\n",
    "start = pd.Timestamp(\"01-01-2016\", freq=\"H\")\n",
    "# train dataset: cut the last window of length \"prediction_length\", add \"target\" and \"start\" fields\n",
    "train_ds = ListDataset([{'target': y.loc[:450,'y'], 'start': start}], freq='H')\n",
    "# test dataset: use the whole dataset, add \"target\" and \"start\" fields\n",
    "test_ds = ListDataset([{'target': y['y'], 'start': start}],freq='H')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-camcorder",
   "metadata": {},
   "source": [
    "## Listing 20-3. Fitting the default DeepAR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.trainer import Trainer\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(7)\n",
    "mx.random.seed(7)\n",
    "\n",
    "estimator = DeepAREstimator(\n",
    "    prediction_length=28,\n",
    "    context_length=100,\n",
    "    freq=’H’,\n",
    "    trainer=Trainer(ctx=\"gpu\", # remove if running on windows\n",
    "                    epochs=5,\n",
    "                    learning_rate=1e-3,\n",
    "                    num_batches_per_epoch=100\n",
    "                   )\n",
    ")\n",
    "\n",
    "predictor = estimator.train(train_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-conspiracy",
   "metadata": {},
   "source": [
    "## Listing 20-4. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-swiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictor.predict(test_ds)\n",
    "predictions = list(predictions)[0]\n",
    "predictions = predictions.quantile(0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-donor",
   "metadata": {},
   "source": [
    "## Listing 20-5. R2 score and prediction graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print(r2_score( list(test_ds)[0]['target'][-28:], predictions))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(predictions)\n",
    "plt.plot(list(test_ds)[0]['target'][-28:])\n",
    "plt.legend(['predictions', 'actuals'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-underground",
   "metadata": {},
   "source": [
    "## Listing 20-6. Probability forecast graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "\n",
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=test_ds,  # test dataset\n",
    "    predictor=predictor,  # predictor\n",
    "    num_samples=100,  # number of sample paths we want for evaluation\n",
    ")\n",
    "\n",
    "forecasts = list(forecast_it)\n",
    "tss = list(ts_it)\n",
    "\n",
    "ts_entry = tss[0]\n",
    "forecast_entry = forecasts[0]\n",
    "\n",
    "def plot_prob_forecasts(ts_entry, forecast_entry):\n",
    "    plot_length = 150\n",
    "    prediction_intervals = (50.0, 90.0)\n",
    "    legend = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals][::-1]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "    ts_entry[-plot_length:].plot(ax=ax)  # plot the time series\n",
    "    forecast_entry.plot(prediction_intervals=prediction_intervals, color='g')\n",
    "    plt.grid(which=\"both\")\n",
    "    plt.legend(legend, loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "plot_prob_forecasts(ts_entry, forecast_entry)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-material",
   "metadata": {},
   "source": [
    "## Listing 20-7. Preparing holidays and reservations data and adding them into the ListDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reservations = pd.read_csv('air_reserve.csv.zip')\n",
    "X_reservations['visit_date'] = pd.to_datetime(X_reservations['visit_datetime']).dt.date\n",
    "X_reservations = pd.DataFrame(X_reservations.groupby('visit_date')['reserve_visitors'].sum())\n",
    "X_reservations = X_reservations.reset_index(drop = False)\n",
    "\n",
    "# Convert to datatime for merging correctly\n",
    "y.date = pd.to_datetime(y.date)\n",
    "X_reservations.visit_date = pd.to_datetime(X_reservations.visit_date)\n",
    "\n",
    "# Merging and filling missing dates with 0\n",
    "y = y.merge(X_reservations, left_on = 'date', right_on =  'visit_date', how = 'left').fillna(0)\n",
    "\n",
    "# Preparing and merging holidays data\n",
    "holidays = pd.read_csv('date_info.csv.zip')\n",
    "holidays.calendar_date = pd.to_datetime(holidays.calendar_date)\n",
    "y = y.merge(holidays, left_on = 'date', right_on = 'calendar_date', how = 'left').fillna(0)\n",
    "\n",
    "# Preparing the ListDatasets\n",
    "\n",
    "train_ds = ListDataset([{\n",
    "    'target': y.loc[:450,'y'],\n",
    "    'start': start,\n",
    "    'feat_dynamic_real': y.loc[:450,['reserve_visitors', 'holiday_flg']].values\n",
    "    }], freq='H')\n",
    "\n",
    "test_ds = ListDataset([{\n",
    "    'target': y['y'], \n",
    "    'start': start, \n",
    "    'feat_dynamic_real': y.loc[:,['reserve_visitors', 'holiday_flg']].values\n",
    "    }],freq='H')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-screen",
   "metadata": {},
   "source": [
    "## Listing 20-8. Same code for fitting a different model: this model contains the two additional regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "mx.random.seed(7)\n",
    "\n",
    "# Build and fit model\n",
    "estimator = DeepAREstimator(\n",
    "    prediction_length=28,\n",
    "    context_length=100,\n",
    "    freq='H',\n",
    "    trainer=Trainer(ctx=\"gpu\", # remove if running on windows\n",
    "                    epochs=5,\n",
    "                    learning_rate=1e-3,\n",
    "                    num_batches_per_epoch=100\n",
    "                   )\n",
    ")\n",
    "\n",
    "predictor = estimator.train(train_ds)\n",
    "\n",
    "# Make Predictions\n",
    "predictions = predictor.predict(test_ds)\n",
    "predictions = list(predictions)[0]\n",
    "predictions = predictions.quantile(0.5)\n",
    "\n",
    "# Compute and print R2 score\n",
    "print(r2_score( list(test_ds)[0]['target'][-28:], predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-hurricane",
   "metadata": {},
   "source": [
    "## Listing 20-9. Tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "mx.random.seed(7)\n",
    "\n",
    "results = []\n",
    "\n",
    "for learning_rate in [1e-4, 1e-2]:\n",
    "  for num_layers in [2, 5]:\n",
    "    for num_cells in [30, 100]:\n",
    "\n",
    "      estimator = DeepAREstimator(\n",
    "          prediction_length=28,\n",
    "          freq=’H’,\n",
    "          trainer=Trainer(ctx=\"gpu\", # remove if on Windows\n",
    "                          epochs=10,\n",
    "                          learning_rate=learning_rate,\n",
    "                          num_batches_per_epoch=100\n",
    "                        ),\n",
    "          num_layers = num_layers,\n",
    "          num_cells = num_cells,\n",
    "      )\n",
    "\n",
    "      predictor = estimator.train(train_ds)\n",
    "\n",
    "      predictions = predictor.predict(test_ds)\n",
    "\n",
    "      r2 = r2_score(list(predictions)[0].quantile(0.5), list(test_ds)[0]['target'][-28:])\n",
    "      result = [learning_rate, num_layers, num_cells, r2]\n",
    "      print(result)\n",
    "      results.append(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
